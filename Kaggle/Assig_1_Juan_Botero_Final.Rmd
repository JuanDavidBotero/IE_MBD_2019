---
title: "1st Assignment"
output: 
  html_document:
    toc: true
    toc_depth: 3
author: Juan David Botero
---

We start by importing the neccessary libraries to plot, analyze and model our data.
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(purrr)
library(tidyr)
library(plyr)
library(dplyr)     # To compute the `union` of the levels.
library(png)       # To include images in this document.
library(knitr)     # To include images inline in this doc.
library(moments)   # Skewness
library(e1071)     # Alternative for Skewness
library(glmnet)    # Lasso
library(caret)     # To enable Lasso training with CV.
library(corrplot)  # Plot correlation
library(RColorBrewer) # Colors
library(FSelector)
library(doParallel);
library(foreach)
library(DataExplorer)
registerDoParallel(cores = detectCores());
```

# Introduction

This assignment focuses on applying the Feature Engineering processes and the Evaluation methods that we have learned in previous sessions to solve a practical scenario: Predict the price of houses.
In particular, we are going to use the experimental scenario proposed by the House Prices Dataset. This dataset includes 79 explanatory variables of residential homes. For more details on the dataset and the competition see <https://www.kaggle.com/c/house-prices-advanced-regression-techniques>.

## What is my goal?
- I want to predict predict the final price of each home (Therefore, this is a regression task).
- I have to clean the dataset to allow its further processing.
- I have to use the feature engineering techniques explained in class to transform the dataset: filtering, wrapper and embedded methods.
- I have to properly apply the evaluation methods and ideas (train, validation, test splitting; cross-validation, chose the proper metric, ..) to understand the real performance of the proposed models, making sure that they will generalize to unseen data (test set).

# Useful Functions

In order to facilitate the evaluation of the impact of the different steps, I am going to place the code for creating a baseline `glm` model in a function. 

```{r message=FALSE, warning=FALSE}
lm.model <- function(training_dataset, validation_dataset, title) {
  # Create a training control configuration that applies a 5-fold cross validation
  train_control_config <- trainControl(method = "repeatedcv", 
                                       number = 5, 
                                       repeats = 1,
                                       returnResamp = "all")
  
  # Fit a glm model to the input training data
  this.model <- train(SalePrice ~ ., 
                       data = training_dataset, 
                       method = "glm", 
                       metric = "RMSE",
                       preProc = c("center", "scale"),
                       trControl=train_control_config)
  
  # Prediction
  this.model.pred <- predict(this.model, validation_dataset)
  this.model.pred[is.na(this.model.pred)] <- 0 # To avoid null predictions
  
  # RMSE of the model
  thismodel.rmse <- sqrt(mean((this.model.pred - validation_dataset$SalePrice)^2))
  
  # Error in terms of the mean deviation between the predicted value and the price of the houses
  thismodel.price_error <- mean(abs((exp(this.model.pred) -1) - (exp(validation_dataset$SalePrice) -1)))

  # Plot the predicted values against the actual prices of the houses
  my_data <- as.data.frame(cbind(predicted=(exp(this.model.pred) -1), observed=(exp(validation_dataset$SalePrice) -1)))
  ggplot(my_data, aes(predicted, observed)) +
    geom_point() + geom_smooth(method = "lm") +
    labs(x="Predicted") +
    ggtitle(ggtitle(paste(title, 'RMSE: ', format(round(thismodel.rmse, 4), nsmall=4), ' --> Price ERROR:', format(round(thismodel.price_error, 0), nsmall=0), 
                          ' €', sep=''))) +  
    scale_x_continuous(labels = scales::comma) + 
    scale_y_continuous(labels = scales::comma)
}
```

Function to split a dataset into training and validation.

```{r}
splitdf <- function(dataframe) {
  set.seed(123)
 	index <- 1:nrow(dataframe)
 	trainindex <- sample(index, trunc(length(index)/1.5))
 	trainset <- dataframe[trainindex, ]
 	testset <- dataframe[-trainindex, ]
 	list(trainset=trainset,testset=testset)
}
```


# Data Reading and preparation
The dataset is offered in two separated fields, one for the training and another one for the test set. 

```{r Load Data}
original_training_data = read.csv(file = file.path("train.csv"))
original_test_data = read.csv(file = file.path("test.csv"))

```

To avoid applying the Feature Engineering process two times (once for training and once for test), you can just join both datasets (using the `rbind` function), apply your FE and then split the datasets again. However, if we try to do join the two dataframes as they are, we will get an error because they do not have the same columns: `test_data` does not have a column `SalePrice`. Therefore, we first create this column in the test set and then we join the data

```{r Joinning datasets}

# original_training_data$GrLivArea = original_training_data[original_training_data$GrLivArea<=40000,]
original_test_data$SalePrice <- 0
dataset <- rbind(original_training_data, original_test_data)
```

Let's now visualize the dataset to see where to begin
```{r Dataset Visualization}
summary(dataset)
str(dataset)
```

We can see some problems just by taking a look to the summary: the dataset has missing values, there are some categorical columns codified as numeric, it has different scales for the feature values. 

# Data Cleaning

The definition of "meaningless" depends on your data and your intuition. A feature can lack any importance because you know for sure that it does not going to have any impact in the final prediction (e.g., the ID of the house). In addition, there are features that could be relevant but present wrong, empty or incomplete values (this is typical when there has been a problem in the data gathering process). For example, the feature `Utilities` present a unique value, consequently it is not going to offer any advantage for prediction.

We remove meaningless features that have >= 99% of features representing the same category.
```{r NA transformation}
dataset <- dataset[,-which(names(dataset) == "Utilities")]
dataset <- dataset[,-which(names(dataset) == "Id")]
dataset <- dataset[,-which(names(dataset) == "Street")]
dataset <- dataset[,-which(names(dataset) == "PoolQC")]
dataset <- dataset[,-which(names(dataset) == "Condition2")]
dataset <- dataset[,-which(names(dataset) == "RoofMatl")]
dataset <- dataset[,-which(names(dataset) == "Heating")]

```

## Hunting NAs

Our dataset is filled with missing values, therefore, before we can build any predictive model we'll clean our data by filling in all NA's with more appropriate values.

Counting columns with null values.

```{r NAs discovery}
na.cols <- which(colSums(is.na(dataset)) > 0)
paste('There are', length(na.cols), 'columns with missing values')
sort(colSums(sapply(dataset[na.cols], is.na)), decreasing = TRUE)
```


### Here we replace missing values in the dataset

For categorical values there are 3 approaches:

- Create a new "None" category and replace the NA's (Only for features that included NA as a category).
- For the rest, impute NA's for the mayority class.
- Give order to the factors that contain information in their ordering sets. Idea taken from the following Kaggle post: https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard  

For numerical values there where 2 approaches:

- For features with NA's > 50 preProcess with bagImpute method.
- For the rest the NA's where replaced with the Median because of the presence of outliers. 

```{r NAs replacement}

# Alley : NA means "no alley access"
dataset$Alley = factor(dataset$Alley, levels=c(levels(dataset$Alley), "None"))
#and replacing the potential NA values in the dataset by that new 'factor' level.
dataset$Alley[is.na(dataset$Alley)] = "None"

# MiscFeature:
dataset$MiscFeature = factor(dataset$MiscFeature, levels=c(levels(dataset$MiscFeature), "None"))
dataset$MiscFeature[is.na(dataset$MiscFeature)] = "None"

# Fence:
dataset$Fence = factor(dataset$Fence, levels=c(levels(dataset$Fence), "None"))
dataset$Fence[is.na(dataset$Fence)] = "None"

# FireplaceQu:
dataset$FireplaceQu = factor(dataset$FireplaceQu, levels=c(levels(dataset$FireplaceQu), "None"))
dataset$FireplaceQu[is.na(dataset$FireplaceQu)] = "None"

# GarageFinish:
dataset$GarageFinish = factor(dataset$GarageFinish, levels=c(levels(dataset$GarageFinish), "None"))
dataset$GarageFinish[is.na(dataset$GarageFinish)] = "None"

# GarageQual:
dataset$GarageQual = factor(dataset$GarageQual, levels=c(levels(dataset$GarageQual), "None"))
dataset$GarageQual[is.na(dataset$GarageQual)] = "None"

# GarageCond:
dataset$GarageCond = factor(dataset$GarageCond, levels=c(levels(dataset$GarageCond), "None"))
dataset$GarageCond[is.na(dataset$GarageCond)] = "None"

# GarageType:
dataset$GarageType = factor(dataset$GarageType, levels=c(levels(dataset$GarageType), "None"))
dataset$GarageType[is.na(dataset$GarageType)] = "None"

# BsmtExposure:
dataset$BsmtExposure = factor(dataset$BsmtExposure, levels=c(levels(dataset$BsmtExposure), "None"))
dataset$BsmtExposure[is.na(dataset$BsmtExposure)] = "None"

# BsmtCond:
dataset$BsmtCond = factor(dataset$BsmtCond, levels=c(levels(dataset$BsmtCond), "None"))
dataset$BsmtCond[is.na(dataset$BsmtCond)] = "None"

# BsmtQual:
dataset$BsmtQual = factor(dataset$BsmtQual, levels=c(levels(dataset$BsmtQual), "None"))
dataset$BsmtQual[is.na(dataset$BsmtQual)] = "None"

# BsmtFinType2:
dataset$BsmtFinType2 = factor(dataset$BsmtFinType2, levels=c(levels(dataset$BsmtFinType2), "None"))
dataset$BsmtFinType2[is.na(dataset$BsmtFinType2)] = "None"

# BsmtFinType1:
dataset$BsmtFinType1 = factor(dataset$BsmtFinType1, levels=c(levels(dataset$BsmtFinType1), "None"))
dataset$BsmtFinType1[is.na(dataset$BsmtFinType1)] = "None"

# MasVnrType:
dataset$MasVnrType[is.na(dataset$MasVnrType)] = "None"

# MasVnrArea:
#summary(dataset$MasVnrArea) 
dataset$MasVnrArea[is.na(dataset$MasVnrArea)] = summary(dataset$MasVnrArea)["Median"]

# MSZoning:
#summary(dataset$MSZoning) #Being the most relevant class the "RL" I'm gping to input the NA's with RL.
dataset$MSZoning[is.na(dataset$MSZoning)] = "RL"

# BsmtFullBath:
#summary(dataset$BsmtFullBath) # Replacing the 2 NA's with the Median
dataset$BsmtFullBath[is.na(dataset$BsmtFullBath)] = summary(dataset$BsmtFullBath)["Median"]

# BsmtHalfBath:
#summary(dataset$BsmtHalfBath) # Replacing the 2 NA's with the Median
dataset$BsmtHalfBath[is.na(dataset$BsmtHalfBath)] = summary(dataset$BsmtHalfBath)["Median"]

# Functional: 
#summary(dataset$Functional) #Being the most relevant class the "Typ" I'm gping to input the NA's with Typ.
dataset$Functional[is.na(dataset$Functional)] = "Typ"

# Exterior1st:
#summary(dataset$Exterior1st)
dataset$Exterior1st[is.na(dataset$Exterior1st)] = "VinylSd"

# Exterior2nd:
#summary(dataset$Exterior2nd)
dataset$Exterior2nd[is.na(dataset$Exterior2nd)] = "VinylSd"

# BsmtFinSF1:
#summary(dataset$BsmtFinSF1) 
dataset$BsmtFinSF1[is.na(dataset$BsmtFinSF1)] = summary(dataset$BsmtFinSF1)["Median"]

# BsmtFinSF2:
#summary(dataset$BsmtFinSF2)
dataset$BsmtFinSF2[is.na(dataset$BsmtFinSF2)] = summary(dataset$BsmtFinSF2)["Median"]

# BsmtUnfSF:
#summary(dataset$BsmtUnfSF)
dataset$BsmtUnfSF[is.na(dataset$BsmtUnfSF)] = summary(dataset$BsmtUnfSF)["Median"]

# TotalBsmtSF:
#summary(dataset$TotalBsmtSF)
dataset$TotalBsmtSF[is.na(dataset$TotalBsmtSF)] = summary(dataset$TotalBsmtSF)["Median"]

# Electrical:
#summary(dataset$Electrical)
dataset$Electrical[is.na(dataset$Electrical)] = "SBrkr"

# KitchenQual:
#summary(dataset$KitchenQual)
dataset$KitchenQual[is.na(dataset$KitchenQual)] = "Gd"

# GarageCars:
#summary(dataset$GarageCars)
dataset$GarageCars[is.na(dataset$GarageCars)] = summary(dataset$GarageCars)["Median"]

# GarageArea:
#summary(dataset$GarageArea)
dataset$GarageArea[is.na(dataset$GarageArea)] = summary(dataset$GarageArea)["Median"]

# SaleType:
#summary(dataset$SaleType)
dataset$SaleType[is.na(dataset$SaleType)] = "WD"

## Here we give order to the factors that contain information in their ordering sets.

# ExterQual
dataset$ExterQual = ordered(factor(dataset$ExterQual, levels=rev(levels(dataset$ExterQual))))

# ExterCond
dataset$ExterCond = ordered(factor(dataset$ExterCond, levels=rev(levels(dataset$ExterCond))))

# HeatingQC
dataset$HeatingQC = ordered(factor(dataset$HeatingQC, levels=rev(levels(dataset$HeatingQC))))

# KitchenQual
dataset$KitchenQual = ordered(factor(dataset$KitchenQual, levels=rev(levels(dataset$KitchenQual))))

# BsmtFinType1
dataset$BsmtFinType1 = ordered(factor(dataset$BsmtFinType1, levels=rev(levels(dataset$BsmtFinType1))))

# BsmtFinType2
dataset$BsmtFinType2 = ordered(factor(dataset$BsmtFinType2, levels=rev(levels(dataset$BsmtFinType2))))

# Functional
dataset$Functional = ordered(factor(dataset$Functional, levels=rev(levels(dataset$Functional))))

# Fence
dataset$Fence = ordered(factor(dataset$Fence, levels=rev(levels(dataset$Fence))))

# BsmtExposure
dataset$BsmtExposure = ordered(factor(dataset$BsmtExposure, levels=rev(levels(dataset$BsmtExposure))))

# PavedDrive
dataset$PavedDrive = ordered(factor(dataset$PavedDrive, levels=rev(levels(dataset$PavedDrive))))

# GarageFinish
dataset$GarageFinish = ordered(factor(dataset$GarageFinish, levels=rev(levels(dataset$GarageFinish))))

# LandSlope
dataset$LandSlope = ordered(factor(dataset$LandSlope, levels=rev(levels(dataset$LandSlope))))

# LotShape
dataset$LotShape = ordered(factor(dataset$LotShape, levels=rev(levels(dataset$LotShape))))

# Alley
dataset$Alley = ordered(factor(dataset$Alley, levels=rev(levels(dataset$Alley))))

# CentralAir
dataset$CentralAir = ordered(factor(dataset$CentralAir, levels=rev(levels(dataset$CentralAir))))

# FireplaceQu
dataset$FireplaceQu = ordered(factor(dataset$FireplaceQu, levels=rev(levels(dataset$FireplaceQu))))

# GarageQual
dataset$GarageQual = ordered(factor(dataset$GarageQual, levels=rev(levels(dataset$GarageQual))))

# GarageCond
dataset$GarageCond = ordered(factor(dataset$GarageCond, levels=rev(levels(dataset$GarageCond))))

# BsmtCond
dataset$BsmtCond = ordered(factor(dataset$BsmtCond, levels=rev(levels(dataset$BsmtCond))))

# BsmtQual
dataset$BsmtQual = ordered(factor(dataset$BsmtQual, levels=rev(levels(dataset$BsmtQual))))



na.cols <- which(colSums(is.na(dataset)) > 0)
paste('There are', length(na.cols), 'columns with missing values')
sort(colSums(sapply(dataset[na.cols], is.na)), decreasing = TRUE)

```

#### preProcess with bagImpute method for LotFrontage (486 NA's) and GarageYrBlt (159 NA's)
```{r preProcess with bagImpute method}

bagImpute_NA <- preProcess(dataset, method = "bagImpute")
dataset <- predict(bagImpute_NA, dataset)

na.cols <- which(colSums(is.na(dataset)) > 0)
paste('There are', length(na.cols), 'columns with missing values')
```

## Factorize features

If we go back to the summary of the dataset we can identify some numerical features that are actually categories: `MSSubClass` and the Year and Month in which the house was sold. What we have to do is to convert them to the proper 'class' or 'type' using the `as.factor` command.

```{r Wrongly Numerical columns as Factors}

dataset$MSSubClass <- as.factor(dataset$MSSubClass)
dataset$MoSold <- as.factor(dataset$MoSold)
dataset$YrSold <- as.factor(dataset$YrSold)

##Ordering factors

# YrSold
dataset$YrSold = ordered(factor(dataset$YrSold, levels=rev(levels(dataset$YrSold))))
```


## Outliers
We will now focus on numerical values. If `NAs` where the natural enemy of categorical values, the main problem with numerical values are outliers (values which largely differ from the rest). Outliers can mislead the training of our models resulting in less accurate models and ultimately worse results.

In this section we seek to identify outliers to then properly deal with them. If we summarize the dataset, we can see variables which "Max." is much larger than the rest of values. These features are susceptible of containing outliers. Nevetheless, the easiest way to detect outliers is visualizing the numerical values; for instance, by `boxploting` the column values.

```{r Outliers Analysis}

# Variables containing mostly 0 values:
plot(dataset$SalePrice, dataset$BsmtFinSF2) 
plot(dataset$SalePrice, dataset$BsmtHalfBath)
plot(dataset$SalePrice, dataset$MiscVal)
plot(dataset$SalePrice, dataset$KitchenAbvGr)
plot(dataset$SalePrice, dataset$EnclosedPorch)
plot(dataset$SalePrice, dataset$LowQualFinSF) 
plot(dataset$SalePrice, dataset$X3SsnPorch)
plot(dataset$SalePrice, dataset$ScreenPorch)
plot(dataset$SalePrice, dataset$PoolArea)  

# Relevant features with outliers
plot(dataset$SalePrice, dataset$LotArea)  # If outliers are removed it affects negativly the model
plot(dataset$SalePrice, dataset$GrLivArea)  # If outliers are removed it affects negativly the model


# After plotting the 9 variables that mostly have "0" values, there appears not to be considerable outliers to remove in these rows, so we are going to keep them as they are.

# dataset <- dataset[,-which(names(dataset) == "BsmtFinSF2")]
# dataset <- dataset[,-which(names(dataset) == "BsmtHalfBath")]
# dataset <- dataset[,-which(names(dataset) == "MiscVal")]
# dataset <- dataset[,-which(names(dataset) == "KitchenAbvGr")]
# dataset <- dataset[,-which(names(dataset) == "EnclosedPorch")]
# dataset <- dataset[,-which(names(dataset) == "LowQualFinSF")]
# dataset <- dataset[,-which(names(dataset) == "X3SsnPorch")]
# dataset <- dataset[,-which(names(dataset) == "ScreenPorch")]
# dataset <- dataset[,-which(names(dataset) == "PoolArea")]

```

After plotting the 9 variables that mostly have "0" values, there appears not to be considerable outliers to remove in these rows, so we are going to keep them as they are because the we want to preserve possible information they contain. 

For the rest of the numerical features we are NOT going to remove the outliers because this has a negative effect on our models predictios. 

```{r Box-Plots}

#dont_remove_outliers <- c("SalePrice", "BsmtFinSF2", "BsmtHalfBath", "MiscVal", "KitchenAbvGr", "EnclosedPorch", "LowQualFinSF", "X3SsnPorch", "ScreenPorch", "PoolArea", "GrLivArea", "LotArea" )

 #Length of the whiskers as multiple of IQR. Defaults to 1.5 but changed to 3 to remove less outliers.

 # for (col in names(dataset)) {
 #   if (is.numeric(dataset[[col]]) && col%in% dont_remove_outliers == F){
 #     print(ggplot(dataset, aes_string(y=col))+ geom_boxplot(width=0.1) + theme(axis.line.x=element_blank(),axis.title.x=element_blank(), axis.ticks.x=element_blank(), axis.text.x=element_blank(),legend.position="none"))
 #     to_remove <- boxplot.stats(dataset[[col]], coef = 5)$out
 #     print(cat("Number of outliers removed", length(to_remove), col))
 #     dataset <- dataset[!dataset[[col]] %in% to_remove, ]
 #   }
 # }


# # Create plots to see the different relationships between the target variable and the different variables
# plot_bar(dataset)
# plot_boxplot(dataset, by = 'SalePrice')
# # As seen there are some variables with some outliers which will be deleted to apply to the model.
# ggplot(train,aes(y=SalePrice,x=GrLivArea))+geom_point()


```

## Feature Creation

### Check correlation
```{r Correlation, message=FALSE, warning=FALSE}

column_types <- sapply(names(dataset), function(x) {
    class(dataset[[x]])
  }
)

not_factors <- c("numeric", "integer")

numeric_columns <- names(column_types[column_types %in% not_factors == T])
correlation <- cor(dataset[,numeric_columns])
corrplot(correlation, type="upper", order="hclust",
         col=brewer.pal(n=8, name="RdYlBu"))

```
There is mostly positive correlations between variables, as expected. For example YearBuilt and GarageYrBlt. I will use Lasso or Ridge do the clean-up the correlations and avoid multicolinearity, one of the assumptions of linear models. 

New Features are created to help the model capture more information.
```{r Feature Creation, message=FALSE, warning=FALSE}

# feture engineering a new feature "TotalFS"
dataset['TotalSF'] <- dataset['TotalBsmtSF'] + dataset['X1stFlrSF'] + dataset['X2ndFlrSF']

# New feature to give more importance to the quality of the house
dataset["TotalQT"] <- dataset["OverallQual"] * dataset["OverallCond"]

# To give more importance to good houses with many rooms
dataset["TotalQT_TRooms"] <- dataset["TotRmsAbvGrd"] * dataset["TotalQT"]

# To give more importance to remodeled houses
dataset['YrBltAndRemod'] <- dataset['YearBuilt'] + dataset['YearRemodAdd']

# To take into account possible seasonality effects

dataset["Season"] <- recode(dataset$MoSold, "12" = "winter", "1" = "winter", "2" = "winter","3" = "spring", "4" = "spring", "5" = "spring", "6" = "summer", "7" = "summer", "8" = "summer", "9" = "autumn", "10" = "autumn", "11" = "autumn")

# To give more relevance to the total square meters of the house

dataset["TotalQT+GrLivArea"] <- dataset['TotalSF'] + dataset["GrLivArea"]

```


## Skewness

First we now need to detect skewness in the Target value and check for the skewness in the rest of the variables. This is important because another assumption of linears models is a normal distribution of the variables, so we need to transform them to make the model perform better.

```{r}
df <- rbind(data.frame(version="price",x=original_training_data$SalePrice),
            data.frame(version="log(price+1)",x=log(original_training_data$SalePrice + 1)))

ggplot(data=df) +
  facet_wrap(~version,ncol=2,scales="free_x") +
  geom_histogram(aes(x=x), bins = 50)

for (col in names(dataset)) {
  if (is.numeric(dataset[[col]])){
    print(ggplot(dataset, aes(x=dataset[[col]]), title(col)) + geom_histogram(bins = 50) +labs(title = col))
  }
}

```

We therefore transform the target value by applying log because it is right skewed.
```{r Log transform the target for official scoring}
# Log transform the target for official scoring
dataset$SalePrice <- log1p(dataset$SalePrice)


```


The same "skewness" observed in the target variable also affects other variables. To facilitate the application of the regression model we are going to also eliminate this skewness. For numeric feature with excessive skewness, perform log transformation

Threshold for the skewness in 0.60. As this was the value that contributed more to have a lower RMSE

```{r New skewness_threshold}
skewness_threshold = 0.60

column_types <- sapply(names(dataset), function(x) {
    class(dataset[[x]])
  }
)

numeric_columns <- names(column_types[column_types %in% not_factors == T])

```

And now, with that information, we need to calculate the skewness of each column whose name is our list of __factor__ (or categorical) features. We use the `sapply` method again, to compute the skewness of each column whose name is in the list of `numeric_columns`.
```{r}
# skew of each variable
skew <- sapply(numeric_columns, function(x) { 
    e1071::skewness(dataset[[x]], na.rm = T)
  }
)

```

What we do need to make now is to apply the log to those whose skewness value is below a given threshold that we've set in 0.60. We should test different hypothesis with our threshold too.

```{r}
# transform all variables above a threshold skewness.
skew <- skew[abs(skew) > skewness_threshold]
for(x in names(skew[!is.na(skew)])) {
  dataset[[x]] <- log(dataset[[x]] + 1)
}

```
## Dummy Variables

To finalize the Feature Engineering proccess dummy variables are created to compare the performance of the linear models and see if it increases performance of the model.

```{r dummy variables}
 
dmy = dummyVars(" ~ .", data = dataset, fullRank = T)
 
dataset_encoded <- data.frame(predict(dmy, newdata = dataset))

training_data_encoded <- dataset_encoded[1:1460,]
test_encoded <- dataset_encoded[1461:2919,]

splitdf <- function(dataframe, seed=NULL) {
  if (!is.null(seed)) set.seed(seed)
 	index <- 1:nrow(dataframe)
 	trainindex <- sample(index, trunc(length(index)/1.5))
 	trainset <- dataframe[trainindex, ]
 	testset <- dataframe[-trainindex, ]
 	list(trainset=trainset,testset=testset)
}
splits_encoded <- splitdf(training_data_encoded, seed=1)
training_encoded <- splits_encoded$trainset
validation_encoded <- splits_encoded$testset
```
This step proved to be irrelevant to increasing the performance because I encoded orderes to the categorical variables and if you have orders it is not recommended to one-hot encode the dataset.


# Train, Validation Spliting

To facilitate the data cleaning and feature engineering we merged train and test datasets. We now split them again to create our final model.

```{r Train test split}
training_data <- dataset[1:1460,]
test <- dataset[1461:2919,]
```

We are going to split the annotated dataset in training and validation for the later evaluation of our regression models
```{r Train Validation split}
# I found this function, that is worth to save for future ocasions.
splitdf <- function(dataframe, seed=NULL) {
  if (!is.null(seed)) set.seed(seed)
 	index <- 1:nrow(dataframe)
 	trainindex <- sample(index, trunc(length(index)/1.5))
 	trainset <- dataframe[trainindex, ]
 	testset <- dataframe[-trainindex, ]
 	list(trainset=trainset,testset=testset)
}
splits <- splitdf(training_data, seed=1)
training <- splits$trainset
validation <- splits$testset
```

# Feature Selection
We here start the Feature Selection.

## Filtering Methods
We will rank the features according to their predictive power according to the methodologies seen in class: the Chi Squared Independence test, Spearman's correlation and the Information Gain.


#### Full Model

Let's try first a baseline including all the features to evaluate the impact of the feature engineering.

```{r message=FALSE, warning=FALSE}
lm.model(training, validation, "Baseline")

```

This is our first lm that we are going to use to compare our more complex models to.

### Chi-squared Selection
This will only measure the relationship between the categorical features and the output.

```{r warning=FALSE}
# Compute the ChiSquared Statistic over the factor features ONLY
features <- names(training[, sapply(training, is.factor) & colnames(training) != 'SalePrice'])
chisquared <- data.frame(features, statistic = sapply(features, function(x) {
  chisq.test(training$SalePrice, training[[x]])$statistic
}))

# Plot the result, and remove those below the 1st IQR (inter-quartile-range) --aggressive
par(mfrow=c(1,2))
boxplot(chisquared$statistic)
bp.stats <- as.integer(boxplot.stats(chisquared$statistic)$stats)   # Get the statistics from the boxplot

chisquared.threshold = bp.stats[3]  # This element represent the 2st quartile.
text(y = bp.stats, labels = bp.stats, x = 1.3, cex=0.7)
barplot(sort(chisquared$statistic), names.arg = chisquared$features, cex.names = 0.6, las=2, horiz = T)
abline(v=chisquared.threshold, col='red')  # Draw a red line over the 1st IQR
```

As we can see on the plot, the feature created "Season" is the most relevant feature for the model, we will now remove 
the features with a Chi Squared test statistic against the output below the 2 IQR, because this value decreases the RMSE of our model.

```{r message=FALSE, warning=FALSE}
# Determine what features to remove from the training set.
features_to_remove_chi <- as.character(chisquared[chisquared$statistic < chisquared.threshold, "features"])
lm.model(training[!names(training) %in% features_to_remove_chi], validation, "ChiSquared Model")
```
With ChiSquared we have a better and simpler model with less features. 

It is up to you to decide whether apply or not this selection based on the achieved results.

### Spearman's correlation.

What to do with the numerical variables? We can always measure its relation with the outcome through the Spearman's correlation coefficient, and remove those with a lower value. Let's repeat the same process we did with the Chi Square but modifying our code to solely select numerical features and measuring Spearman'.

```{r}

# Compute the Spearman Correlation over the numerical features ONLY
features <- names(training[, sapply(training, is.numeric) & colnames(training) != 'SalePrice'])

spearman <- data.frame(features, statistic = sapply(features, function(x) {
  cor(training$SalePrice, training[[x]], method='spearman')
}))

# Plot the result, and remove those below the 1st IQR (inter-quartile-range) --aggressive
par(mfrow=c(1,2))
boxplot(abs(spearman$statistic))
bp.stats <- boxplot.stats(abs(spearman$statistic))$stats   # Get the statistics from the boxplot
text(y = bp.stats, 
     labels = sapply(bp.stats, function(x){format(round(x, 3), nsmall=3)}), # This is to reduce the nr of decimals
     x = 1.3, cex=0.7)

spearman.threshold = bp.stats[2]  # This element represent the 1st quartile.

barplot(sort(abs(spearman$statistic)), names.arg = spearman$features,  cex.names = 0.6, las=2, horiz = T)
abline(v=spearman.threshold, col='red')  # Draw a red line over the 1st IQR
```

As we have seen in the previous model the numerical features created have a great importance to the model.

Let's train the model with the new features, exactly as we did in the Chi Sq. section above.

```{r message=FALSE, warning=FALSE}
# Determine what features to remove from the training set.
features_to_remove_spearman <- as.character(spearman[spearman$statistic < spearman.threshold, "features"])
lm.model(training[!names(training) %in% features_to_remove_spearman], validation, "Spearman's correlation Model")

```

Again, you have to decide if this selection is worthy, the final decision is yours.

```{r message=FALSE, warning=FALSE}

features_to_remove_chi_spearman <- c(features_to_remove_chi, features_to_remove_spearman)
lm.model(training[!names(training) %in% features_to_remove_chi_spearman], validation, "with only the relevant features")

```
As we can see by applying the features selected by our models we already get an improvement with a simple linear model.


### Information Gain Selection

"This Filter Method helps to understand how important the inclusion or exclusion of a particular feature is in predicting. 
It estimates the 'amount of information' that is shared between two random variables say X and Y."- Ángel Castellanos-Featuring Engineering Slides.

```{r warning=FALSE}
# Compute the Information.Gain Statistic 
weights<- data.frame(information.gain(SalePrice~., training))
weights$feature <- rownames(weights)
weights[order(weights$attr_importance, decreasing = TRUE),]
information_gain_features <- weights$feature[weights$attr_importance > 0.2]

```

Now, we can test if this a good move, by using only the features selected by Information Gain > 0.2.

```{r message=FALSE, warning=FALSE}

lm.model(training[c(information_gain_features, "SalePrice")], validation, "Information Gain")
```
Comparing the different filtering methods the one with the best output is the Information Gain filtering with > 0.2, this is the features that are going to be used for the final model.


## Wrapper Methods

Experiment now with Wrapper Methods and select what is the best possible compromise between the number of predictors and the results obtained.

### Stepwise Selection
Combinations of features are computed, evaluated and compared to other combinations.

*This code takes to much time to load so I comment it so it doesnt run.*
```{r Stepwise Selection, warning=FALSE, eval = FALSE}

train_control_config_4_stepwise <- trainControl(method = "none")

backward.lm.mod <- train(SalePrice ~ ., data = training,
               method = "glmStepAIC",
               direction = "backward",
               trace = FALSE,
               metric = "RMSE",
               steps = 15,
               preProc = c("center", "scale"),
               trControl=train_control_config_4_stepwise)




```
Printout only the selected features.
```{r, eval = FALSE}
paste("Features Selected" ,backward.lm.mod$finalModel$formula[3])

lm.model(training_encoded[backward.lm.mod$finalModel$xNames], validation_encoded[backward.lm.mod$finalModel$xNames], "Backward StepWise")

backward.lm.mod.pred <- predict(backward.lm.mod, validation[,-which(names(test) == "SalePrice")])

```
*I was not able to run the code to plot the lm with the features selected in the Backward Stepwise...*

### PCA

To try and perform dimentionality reduction and see if it improves our model.
```{r PCA, warning=FALSE}

#prin_comp <- prcomp(dataset[numeric_columns[numeric_columns != 'SalePrice']], center = TRUE, scale. = TRUE)
prin_comp <- prcomp(dataset_encoded, center = TRUE, scale. = TRUE)
class(prin_comp)
str(prin_comp)

#remove_after_pca <- numeric_columns[numeric_columns != 'SalePrice']
remove_after_pca <- colnames(dataset_encoded[colnames(dataset_encoded) != 'SalePrice'])

# Get dataset after PCA
pca_dat <- prin_comp$x

# Choose number of variables using explained variance
prop_variance_explained <- summary(prin_comp)$importance[3,];
threshold_variance <- 0.9; # Threshold selected
number_of_variables <- min(which(prop_variance_explained > threshold_variance))
final_pca_dat <- pca_dat[,1:number_of_variables];
head(final_pca_dat)

# appendding the PCA's calculated 
dataset2 <- dataset_encoded
dataset_pca <- cbind(dataset2,final_pca_dat) 

dataset_pca <- dataset_pca[colnames(dataset_pca) != remove_after_pca]

```

Test PCA features on the lm

```{r message=FALSE, warning=FALSE}
training_data_pca <- dataset_pca[1:1460,]
test_pca <- dataset_pca[1461:2919,]

# I found this function, that is worth to save for future ocasions.
splitdf <- function(dataframe, seed=NULL) {
  if (!is.null(seed)) set.seed(seed)
 	index <- 1:nrow(dataframe)
 	trainindex <- sample(index, trunc(length(index)/1.5))
 	trainset <- dataframe[trainindex, ]
 	testset <- dataframe[-trainindex, ]
 	list(trainset=trainset,testset=testset)
 	}
splits <- splitdf(training_data_pca, seed=1)
training_pca <- splits$trainset
validation_pca <- splits$testset

lm.model(training_pca, validation_pca, "PCA Model")


```
There is no real improvemnt using the PCA so I will not take it into account.

## Embedded
Finally, we will experiment with embedded methods.
"Learn which features best contribute to the accuracy of the model while the model is being created."- Ángel Castellanos-Featuring Engineering Slides.

### Ridge Regression

For this exercise, we are going to make use of the <a href="https://cran.r-project.org/web/packages/glmnet/index.html">`glmnet`</a> library. Take a look to the library to fit a glmnet model for Ridge Regression, using a grid of lambda values.

```{r Ridge Regression, warning=FALSE}
lambdas <- 10^seq(-3, 0, by = .05)

set.seed(121)
train_control_config <- trainControl(method = "repeatedcv", 
                                     number = 5, 
                                     repeats = 1,
                                     returnResamp = "all")

ridge.mod <- train(SalePrice ~ ., data = training, 
               method = "glmnet", 
               metric = "RMSE",
               trControl=train_control_config,
               tuneGrid = expand.grid(alpha = 0, lambda = lambdas))
```

#### Evaluation

Plotting the RMSE for the different lambda values, we can see the impact of this parameter in the model performance.
Small values seem to work better for this dataset.

```{r Ridge RMSE}
plot(ridge.mod)
```

```{r Ridge Best Lambda}
bestlam <- ridge.mod$bestTune['lambda']
paste("Best Lambda value from CV=", bestlam)
```
Plotting the coefficients for different lambda values. As expected the larger the lambda (lower Norm) value the smaller the coefficients of the features. Ridge Regression does not remove any feature, it just forces the coefficients to take small values.

```{r Ridge Coefficients}
plot(ridge.mod$finalModel)
```


```{r Ridge Evaluation}

ridge.mod.pred <- predict(ridge.mod, validation)
ridge.mod.pred[is.na(ridge.mod.pred)] <- 0

my_data <- as.data.frame(cbind(predicted=(exp(ridge.mod.pred) -1), observed=(exp(validation$SalePrice) -1)))
ridge.mod.rmse <- sqrt(mean((ridge.mod.pred - validation$SalePrice)^2))
ridge.mod.price_error <- mean(abs((exp(ridge.mod.pred) -1) - (exp(validation$SalePrice) -1)))

ggplot(my_data, aes(predicted, observed)) +
    geom_point() + geom_smooth(method = "glm") +
    labs(x="Predicted") +
    ggtitle(ggtitle(paste("Ridge", 'RMSE: ', format(round(ridge.mod.rmse, 4), nsmall=4), ' --> Price ERROR:', format(round(ridge.mod.price_error, 0), nsmall=0), 
                        ' €', sep=''))) +  
    scale_x_continuous(labels = scales::comma) + 
    scale_y_continuous(labels = scales::comma)

```


Rank the variables according to the importance attributed by the model.
```{r}
# Print, plot variable importance
plot(varImp(ridge.mod), top = 20) # 20 most important features
```
Here is a bit counter intuitive why a feature like Exterior1stBrkComm would have such a big impact in the model, being a not so relevant feature for our previous models.

### Lasso Regresion

The only thing that changes between Lasso and Ridge is the `alpha` parameter = 1. 
```{r Lasso Regression, warning=FALSE}
lambdas <- 10^seq(-5, -2, by = .05)

set.seed(121)
train_control_config <- trainControl(method = "repeatedcv", 
                                     number = 5, 
                                     repeats = 1,
                                     returnResamp = "all")

ridge.mod <- train(SalePrice ~ ., data = training, 
               method = "glmnet", 
               metric = "RMSE",
               trControl=train_control_config,
               tuneGrid = expand.grid(alpha = 1, lambda = lambdas))



```

Plotting the RMSE for the different lambda values
```{r Lasso RMSE}
plot(ridge.mod)

```
```{r Lasso Best Lambda}
bestlam <- ridge.mod$bestTune['lambda']
paste("Best Lambda value from CV=", bestlam)
```

Plot coefficients
```{r Lasso Coefficients}
plot(ridge.mod$finalModel)
```
Model evaluation
```{r Lasso Evaluation}

ridge.mod.pred <- predict(ridge.mod, validation)
ridge.mod.pred[is.na(ridge.mod.pred)] <- 0

my_data <- as.data.frame(cbind(predicted=(exp(ridge.mod.pred) -1), observed=(exp(validation$SalePrice) -1)))
ridge.mod.rmse <- sqrt(mean((ridge.mod.pred - validation$SalePrice)^2))
ridge.mod.price_error <- mean(abs((exp(ridge.mod.pred) -1) - (exp(validation$SalePrice) -1)))

ggplot(my_data, aes(predicted, observed)) +
    geom_point() + geom_smooth(method = "glm") +
    labs(x="Predicted") +
    ggtitle(ggtitle(paste("Lasso", 'RMSE: ', format(round(ridge.mod.rmse, 4), nsmall=4), ' --> Price ERROR:', format(round(ridge.mod.price_error, 0), nsmall=0), 
                        ' €', sep=''))) +  
    scale_x_continuous(labels = scales::comma) + 
    scale_y_continuous(labels = scales::comma)

```
Rank Feature importance
```{r}
# Print, plot variable importance
plot(varImp(ridge.mod), top = 20) # 20 most important features
```
Our Lasso model outperforms the Ridge because it removes features that are not relevant for the model, by making their coefficients equal to 0. Here it makes more business sense that that the feature "TotalQT+GrLivArea" is the #1 feature for the model.

### Elastic Net

 `alpha` parameter = 0.5. Combination of Lasso and Ridge to check if the results are improved.

```{r Elastic Net Regression, warning=FALSE}
lambdas <- 10^seq(-5, -2, by = .05)

set.seed(121)
train_control_config <- trainControl(method = "repeatedcv", 
                                     number = 5, 
                                     repeats = 1,
                                     returnResamp = "all")

ridge.mod <- train(SalePrice ~ ., data = training, 
               method = "glmnet", 
               metric = "RMSE",
               trControl=train_control_config,
               tuneGrid = expand.grid(alpha = 0.5, lambda = lambdas))



```

Plotting the RMSE for the different lambda values
```{r Elastic Net RMSE}
plot(ridge.mod)

```
```{r Elastic Net Best Lambda}
bestlam <- ridge.mod$bestTune['lambda']
paste("Best Lambda value from CV=", bestlam)
```


Plot coefficients
```{r Elastic Net Coefficients}
plot(ridge.mod$finalModel)
```
Model evaluation
```{r Elastic Net Evaluation}

ridge.mod.pred <- predict(ridge.mod, validation)
ridge.mod.pred[is.na(ridge.mod.pred)] <- 0

my_data <- as.data.frame(cbind(predicted=(exp(ridge.mod.pred) -1), observed=(exp(validation$SalePrice) -1)))
ridge.mod.rmse <- sqrt(mean((ridge.mod.pred - validation$SalePrice)^2))
ridge.mod.price_error <- mean(abs((exp(ridge.mod.pred) -1) - (exp(validation$SalePrice) -1)))

ggplot(my_data, aes(predicted, observed)) +
    geom_point() + geom_smooth(method = "glm") +
    labs(x="Predicted") +
    ggtitle(ggtitle(paste("Elastic Net", 'RMSE: ', format(round(ridge.mod.rmse, 4), nsmall=4), ' --> Price ERROR:', format(round(ridge.mod.price_error, 0), nsmall=0), 
                        ' €', sep=''))) +  
    scale_x_continuous(labels = scales::comma) + 
    scale_y_continuous(labels = scales::comma)

```
Rank Feature importance
```{r}
# Print, plot variable importance
plot(varImp(ridge.mod), top = 20) # 20 most important features
```

# Final Submission

For the final solution I tried different models using the features selected in the Information Gain exercise, also training the Elastic Net with the one-hot encoded dataset and the PCA dataset, but there was no improvement in the submission score in Kaggle based on the code in this Markdown. 

My final score was: 0.12788 using the Elastic Net.

I think there must be a way of improving the model by removing some outliers, and try to find the point the model is overlly predicting (bottom right) to understand and fix the model in order to achieve a better prediction. 


```{r Final Submission}

# Train the model using all the data
final.model <- train(SalePrice ~ ., data = training, 
               method = "glmnet", 
               metric = "RMSE",
               trControl=train_control_config,
               tuneGrid = expand.grid(alpha = 0.5, lambda = lambdas))

# Predict the prices for the test data (i.e., we use the exp function to revert the log transformation that we applied to the target variable)
final.pred <- as.numeric(exp(predict(final.model, test))-1) 
final.pred[is.na(final.pred)]
hist(final.pred, main="Histogram of Predictions", xlab = "Predictions")

lasso_submission <- data.frame(Id = original_test_data$Id, SalePrice= (final.pred))
colnames(lasso_submission) <-c("Id", "SalePrice")
write.csv(lasso_submission, file = "submission.csv", row.names = FALSE) 


```


